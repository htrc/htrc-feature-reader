{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from htrc_features import Volume, transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model with Gensim\n",
    "\n",
    "The easiest way is using the downloader API, but you can also load locally downloaded models.\n",
    "\n",
    "Just for the demonstration, I'm using the smalled GloVe model, the 25 dimension Twitter model. I highly encouage the model's trained on the Gigaword wiki corpus, and with more dimension (e.g. try `'glove-wiki-gigaword-300'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.1768  ,  0.82329 , -0.19366 , -0.25328 ,  0.99367 , -0.1751  ,\n",
       "        0.95619 , -0.14049 ,  0.90307 ,  0.77942 ,  0.052748,  0.015829,\n",
       "       -3.0639  ,  0.79883 ,  0.97166 ,  0.1536  ,  0.54858 , -0.062755,\n",
       "       -1.1394  , -0.53928 , -0.49389 , -0.17549 , -0.41542 ,  0.62815 ,\n",
       "       -0.33548 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = api.load('glove-twitter-25')\n",
    "model['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Volume and transforming it to WEM\n",
    "\n",
    "Here is a quick way to load a volume. *This loads directly from the HTRC's servers* and shoeldn't be done at scale. At scale, load local files that you've rsynced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/nyp.33433061424580'>The works of Sir William Jones : with the life of the author by Lord Teignmouth.  In thirteen volumes.</a></strong> by <em>Jones, William, Sir, 1746-1794.</em> (1807, 648 pages) - <code>nyp.33433061424580</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f03813c0208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = Volume('nyp.33433061424580')\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Feature Reader library has a function called `transformations.chunk_to_wem`, which gives you word embeddings for a EF tokenlist, either by page or by 'chunk' (which is a grouping of words from multiple pages, aimed at a target length). You can use stoplisting with `stop=True`, which requires the SpaCy library, and you can log transform the token counts so that very common words are not overly represented. The final vector per page/chunk is an average of all the word vectors, weighted by the (optionally log-transformed) word counts.\n",
    "\n",
    "You can also set a `min_count`, which filters out infrequent words, and provide a vocabulary of words in the model, which will speed up the code a bit. Note that the default *n_count* is tuned for 10k word chunks, at the page level it might be sensible to set to 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_to_wem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchunk_tl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_ncount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Take a file that has ['token', 'count'] data and convert to a WEM vector\n",
       "\u001b[0;31mFile:\u001b[0m      ~/htrc-feature-reader/htrc_features/transformations.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformations.chunk_to_wem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41950</th>\n",
       "      <td>353</td>\n",
       "      <td>w</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>21</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11357</th>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page token  count\n",
       "41950   353     w      1\n",
       "1414     21  male      3\n",
       "11357    92     4      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenlist = vol.tokenlist(pos=False, drop_section=True).reset_index()\n",
    "tokenlist.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 7 Vector: [ 0.7371707  -0.78592011 -0.2511647   0.17435114 -0.08399114  0.5422133\n",
      "  0.87715426  1.88172583 -0.45720661  1.03818983  0.37447861 -0.60314632\n",
      " -3.32686159  0.22923918 -0.22501893 -0.74857824 -0.79556065 -0.02725386\n",
      " -1.11331707 -0.60854221  0.25328588 -0.16529209 -0.51196496  0.46722921\n",
      "  0.47008771]\n",
      "Breaking loop, just showing a single example\n"
     ]
    }
   ],
   "source": [
    "for page, group in tokenlist.groupby('page'):\n",
    "    vec = transformations.chunk_to_wem(group, model, vocab=None, stop=False, log=True, min_ncount=1)\n",
    "    print(\"Page:\", page, \"Vector:\", vec)\n",
    "    print(\"Breaking loop, just showing a single example\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example using 5000 word chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31799</th>\n",
       "      <td>25</td>\n",
       "      <td>inevitable</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>11</td>\n",
       "      <td>times</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25324</th>\n",
       "      <td>20</td>\n",
       "      <td>published</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chunk       token  count\n",
       "31799     25  inevitable      4\n",
       "15603     11       times      1\n",
       "25324     20   published      4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenlist = vol.tokenlist(chunk=True, chunk_target=5000, pos=False, drop_section=True).reset_index()\n",
    "tokenlist.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 1 Vector: [-0.19062095  0.233643   -0.14757691  0.08613188 -0.05712432 -0.15239129\n",
      "  0.58771984 -0.58536827 -0.07212887 -0.120707   -0.0931105   0.26524463\n",
      " -3.18245603  0.24303958 -0.01725546  0.07850345  0.2587761   0.01840434\n",
      "  0.24959177 -0.25468259 -0.03151951  0.23090205 -0.17642475 -0.15148901\n",
      " -0.30538046]\n"
     ]
    }
   ],
   "source": [
    "for chunk, group in tokenlist.groupby('chunk'):\n",
    "    vec = transformations.chunk_to_wem(group, model, vocab=None, stop=False, log=True, min_ncount=1)\n",
    "    print(\"Chunk:\", chunk, \"Vector:\", vec)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
