{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTRC Features Feader 2.0 Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Volume object used to handle JSON parsing and feature logic, while the FeatureReader handled reading and decompression.\n",
    "\n",
    "This was recently updated, to disentangle reading and parsing of dataset files from working with them.  Volume now outsources to a set of parser functions - by default the 'jsonVolumeParser' - allowing for alternative versions of the Extracted Features Dataset to be stored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for Extracted Features 2.0\n",
    "\n",
    "The new release of the HTRC Extracted Features Dataset changes the JSON format slightly, to use JSON-LD and be mostly Schema.org compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded mdp.39015014589116 (Labor unions and autocracy in Iran /) from the new EF2.0 features schema https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\n"
     ]
    }
   ],
   "source": [
    "vol = Volume(path='../data/ef2-stubby/mdp/31181/mdp.39015014589116.json.bz2')\n",
    "print(\"Successfully loaded {} ({}) from the new EF2.0 \"\n",
    "      \"features schema {}\".format(vol.id, vol.title, vol.parser._schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features, such as the tokencounts in `vol.tokenlist`, are available. Note that the HathiTrust Reserach Center improved the tokenization and part-of-speech tagging, so the results may vary slightly from prior versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-section of Labor unions and autocracy in Iran /: Page 40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">body</th>\n",
       "      <th>'</th>\n",
       "      <th>''</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <th>''</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <th>POS</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <th>,</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <th>:</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <th>IN</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workers</th>\n",
       "      <th>NNS</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <th>MD</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yet</th>\n",
       "      <th>RB</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "section token   pos       \n",
       "body    '       ''       2\n",
       "        ''      ''       5\n",
       "        's      POS      3\n",
       "        ,       ,       35\n",
       "        --      :        4\n",
       "...                    ...\n",
       "        with    IN       6\n",
       "        workers NNS      3\n",
       "        would   MD       1\n",
       "        year    NN       1\n",
       "        yet     RB       1\n",
       "\n",
       "[234 rows x 1 columns]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = vol.tokenlist()\n",
    "print(\"cross-section of {}: Page 40\".format(vol.title))\n",
    "token_counts.xs(40, level='page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One new change is that `sentenceCount` is not provided when there is no language model or if sentences cannot be parsed from the page - then the value is shown as `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenCount</th>\n",
       "      <th>lineCount</th>\n",
       "      <th>emptyLineCount</th>\n",
       "      <th>capAlphaSeq</th>\n",
       "      <th>sentenceCount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>238</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>144</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>793</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>880</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>228</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokenCount  lineCount  emptyLineCount  capAlphaSeq  sentenceCount\n",
       "page                                                                   \n",
       "2             55         29               6            1            NaN\n",
       "5             12          2               0            1            1.0\n",
       "6            238         52              10            3            NaN\n",
       "7             14          6               0            3            1.0\n",
       "8            144         21               0            4           17.0\n",
       "...          ...        ...             ...          ...            ...\n",
       "350          793         96               0           17            4.0\n",
       "351          880         95               0            9            5.0\n",
       "352          228         32               0           19            4.0\n",
       "353           93         10               0            3            1.0\n",
       "359            1          2               1            1            NaN\n",
       "\n",
       "[344 rows x 5 columns]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.section_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the algorithmically inferred language on each page is now under `calculatedLanguage`, for clarity, and offers the top language rather the full list of language probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page\n",
       "1      None\n",
       "2        gl\n",
       "3      None\n",
       "4      None\n",
       "5        en\n",
       "       ... \n",
       "356    None\n",
       "357    None\n",
       "358    None\n",
       "359      in\n",
       "360    None\n",
       "Name: calculatedLanguage, Length: 360, dtype: object"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.page_features('calculatedLanguage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EF2.0 Metadata\n",
    "\n",
    "Most of the Metadata is schema.org compatible, and is mapped to attributes of Volume.\n",
    "![Screenshot of auto-fill attributes shown for a volume, showing many metadata fields](images/metadata.png)\n",
    "\n",
    "Generally, field names match the schema's key, converted from CamelCase to a snake_case, which is the preferred convention for attributes. One exception is that `schemaVersion` is renamed to `metadata_schema_version`, for clarity next to `features_schema_version`. Fields use their name from the EF2.0 schema, rather than trying to map on to the field names of past versions.\n",
    "\n",
    "All the metadata can be returned from the volume's parser, as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'mdp.39015014589116',\n",
       " 'metadata_schema_version': 'https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0',\n",
       " 'enumeration_chronology': None,\n",
       " 'type_of_resource': 'http://id.loc.gov/ontologies/bibframe/Text',\n",
       " 'title': 'Labor unions and autocracy in Iran /',\n",
       " 'date_created': 20200209,\n",
       " 'pub_date': 1985,\n",
       " 'language': 'eng',\n",
       " 'access_profile': 'google',\n",
       " 'isbn': '0815623437',\n",
       " 'issn': None,\n",
       " 'lccn': '85017300',\n",
       " 'oclc': '12420719',\n",
       " 'page_count': 360,\n",
       " 'feature_schema_version': 'https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0',\n",
       " 'access_rights': 'ic',\n",
       " 'alternate_title': None,\n",
       " 'category': 'Industries. Land use. Labor',\n",
       " 'genre_ld': ['http://id.loc.gov/vocabulary/marcgt/doc',\n",
       "  'http://id.loc.gov/vocabulary/marcgt/bib'],\n",
       " 'genre': ['document (computer)', 'bibliography'],\n",
       " 'contributor_ld': {'id': 'http://www.viaf.org/viaf/24638157',\n",
       "  'type': 'http://id.loc.gov/ontologies/bibframe/Person',\n",
       "  'name': 'Ladjevardi, Habib.'},\n",
       " 'contributor': 'Ladjevardi, Habib.',\n",
       " 'handle_url': 'http://hdl.handle.net/2027/mdp.39015014589116',\n",
       " 'source_institution_ld': {'type': 'http://id.loc.gov/ontologies/bibframe/Organization',\n",
       "  'name': 'MIU'},\n",
       " 'source_institution': 'MIU',\n",
       " 'lcc': 'HD6805.2.L33 1985',\n",
       " 'type': ['DataFeedItem', 'Book'],\n",
       " 'is_part_of': None,\n",
       " 'last_rights_update_date': 20191108,\n",
       " 'pub_place_ld': {'id': 'http://id.loc.gov/vocabulary/countries/nyu',\n",
       "  'type': 'http://id.loc.gov/ontologies/bibframe/Place',\n",
       "  'name': 'New York (State)'},\n",
       " 'pub_place': 'New York (State)',\n",
       " 'main_entity_of_page': ['https://catalog.hathitrust.org/Record/000422656',\n",
       "  'http://catalog.hathitrust.org/api/volumes/brief/oclc/12420719.json',\n",
       "  'http://catalog.hathitrust.org/api/volumes/full/oclc/12420719.json'],\n",
       " 'publisher_ld': {'id': 'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Syracuse%20University%20Press',\n",
       "  'type': 'http://id.loc.gov/ontologies/bibframe/Organization',\n",
       "  'name': 'Syracuse University Press'},\n",
       " 'publisher': 'Syracuse University Press'}"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.parser.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the above that when a value for a field is a higher level datatype, like a Schema.org Organization or VIAF Person, the library renames that field to `field_ld` and extracts the name of the entity into `field`. Let's look more closely at publisher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Syracuse University Press'"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Syracuse%20University%20Press',\n",
       " 'type': 'http://id.loc.gov/ontologies/bibframe/Organization',\n",
       " 'name': 'Syracuse University Press'}"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.publisher_ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real linked data entity is the latter, while the plain text version that some users may look for is the former.\n",
    "\n",
    "Note that the Feature Reader still maintains a few attributes remapping common metadata to friendlier names - `Volume.year` returns the metadata in `pub_date` and `Volume.author` returns the metadata for `contributor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Ladjevardi, Habib.'], 1985)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.author, vol.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notable new metadata fields in Extracted Features 2.0 are 'category' and 'genre'. Genre provides more information about the type of volume, and can help in focusing on only certain holdings of the HathiTrust Digital Library. For example, you may want to exclude goverment works by looking for `government publication`. `vol.genre` provides the plain text description of the genre(s) as a list (as per the [schema](http://id.loc.gov/vocabulary/marcgt.html), while vol.genre_ld provided the persistent identifier for the authority record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['document (computer)', 'bibliography'],\n",
       " ['http://id.loc.gov/vocabulary/marcgt/doc',\n",
       "  'http://id.loc.gov/vocabulary/marcgt/bib'])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.genre, vol.genre_ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`category` provides the name of the Library of Congress subclass of the book, when a classification number is present. e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Industries. Land use. Labor', 'HD6805.2.L33 1985')"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.category, vol.lcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards compatibility\n",
    "\n",
    "The Feature Reader still supports older EF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/hvd.hwquxe'>The man from Glengarry : a tale of the Ottawa / by Ralph Connor.</a></strong> by <em>Connor, Ralph 1860-1937 ,Hazenplug, Frank 1874-1931 binding designer. ,Fleming H. Revell Company publisher. </em> (1901, 482 pages) - <code>hvd.hwquxe</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f3714fccb00>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = Volume('../data/ef1.5-examples/hvd.hwquxe.json.bz2')\n",
    "print(vol.metadata_schema_version)\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for the ID-based retrieval (including the new Stubbytree format)\n",
    "\n",
    "Part of the new HTRC Feature Reader changes allows you to return Extracted Features files by ID, as long as the library knows where to look for them - i.e. are your files in a single directory, or a zip file, or deeper file structure? \n",
    "\n",
    "### Stubbytree (new)\n",
    "\n",
    "The new Extracted Features 2.0 release is contained in a file structure called Stubbytree, an adjustment on the previous Pairtree format which is much gentler on your operating system. The new Feature Reader supports this structure.\n",
    "\n",
    "Consider the file `hvd.32044093320364`. EF files are contained in `../data/ef2-stubby/`, and the file itself is in that structure is at `hvd/34926/hvd.32044093320364.json.bz2`. To load the file with by ID, you tell the library the root directory, with `dir`, and the way of resolving the id (i.e. `id_resolver=\"stubbytree\"`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/hvd.32044093320364'>The Nautilus.</a></strong> by <em>D,e,l,a,w,a,r,e, ,M,u,s,e,u,m, ,o,f, ,N,a,t,u,r,a,l, ,H,i,s,t,o,r,y,.</em> (1904, 222 pages) - <code>hvd.32044093320364</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f370c964be0>"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = Volume('hvd.32044093320364', dir='../data/ef2-stubby/', id_resolver='stubbytree')\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The older `pairtree` format is also supported, with `id_resolver='pairtree'`.\n",
    "\n",
    "### Local Directory\n",
    "\n",
    "If all your files are in the same directory, you can use `id_resolver=\"local\"`. It is also the default assumption with id_resolver isn't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/hvd.hwquxe'>The man from Glengarry : a tale of the Ottawa / by Ralph Connor.</a></strong> by <em>Connor, Ralph 1860-1937 ,Hazenplug, Frank 1874-1931 binding designer. ,Fleming H. Revell Company publisher. </em> (1901, 482 pages) - <code>hvd.hwquxe</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f37136c4c88>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Volume('hvd.hwquxe', dir='../data/ef1.5-examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Stubbytree utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a function for converting an HTID to a Stubbytree path in `utils`. Where an HTID is comprised of a libid and volid, the location is `libid/volid[::3]/` - that is, a subfolder named after every third character of volid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uc2/a+30644/uc2.ark+=13960=t6c24s04.json.bz2'"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from htrc_features import utils\n",
    "utils.id_to_stubbytree('uc2.ark:/13960/t6c24s04', format='json', compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function by default is used in `utils.id_to_rsync`. You can specify a pairtree format if you need it, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uc2/a+30644/uc2.ark+=13960=t6c24s04.json.bz2'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.id_to_rsync('uc2.ark:/13960/t6c24s04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uc2/pairtree_root/ar/k+/=1/39/60/=t/6c/24/s0/4/ark+=13960=t6c24s04/uc2.ark+=13960=t6c24s04.json.bz2'"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.id_to_rsync('uc2.ark:/13960/t6c24s04', format='pairtree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command-line utility, `htid2rsync`, now uses stubbytree by default, and has an `--oldstyle` flag to use the pairtree. *(text preceded with a `!` in this documentation is run on the command line rather than python)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coo/32086/coo.31924109784268.json.bz2\n",
      "hvd/34926/hvd.32044093320364.json.bz2\n",
      "ien/35375/ien.35556031376650.json.bz2\n",
      "keio/1139/keio.10810734990.json.bz2\n",
      "loc/a+30679/loc.ark+=13960=t6737fd9d.json.bz2\n",
      "nyp/33759/nyp.33433070251792.json.bz2\n",
      "nyp/33054/nyp.33433001051444.json.bz2\n",
      "osu/33003/osu.32435005003835.json.bz2\n",
      "osu/33022/osu.32435001924323.json.bz2\n",
      "txu/01171/txu.059172143771152.json.bz2\n"
     ]
    }
   ],
   "source": [
    "!head ../data/ef2-stubby/htids.txt | htid2rsync --from-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coo/pairtree_root/31/92/41/09/78/42/68/31924109784268/coo.31924109784268.json.bz2\n",
      "hvd/pairtree_root/32/04/40/93/32/03/64/32044093320364/hvd.32044093320364.json.bz2\n",
      "ien/pairtree_root/35/55/60/31/37/66/50/35556031376650/ien.35556031376650.json.bz2\n",
      "keio/pairtree_root/10/81/07/34/99/0/10810734990/keio.10810734990.json.bz2\n",
      "loc/pairtree_root/ar/k+/=1/39/60/=t/67/37/fd/9d/ark+=13960=t6737fd9d/loc.ark+=13960=t6737fd9d.json.bz2\n",
      "nyp/pairtree_root/33/43/30/70/25/17/92/33433070251792/nyp.33433070251792.json.bz2\n",
      "nyp/pairtree_root/33/43/30/01/05/14/44/33433001051444/nyp.33433001051444.json.bz2\n",
      "osu/pairtree_root/32/43/50/05/00/38/35/32435005003835/osu.32435005003835.json.bz2\n",
      "osu/pairtree_root/32/43/50/01/92/43/23/32435001924323/osu.32435001924323.json.bz2\n",
      "txu/pairtree_root/05/91/72/14/37/71/15/2/059172143771152/txu.059172143771152.json.bz2\n"
     ]
    }
   ],
   "source": [
    "!head ../data/ef2-stubby/htids.txt | htid2rsync --from-file --oldstyle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting Pairtree to Stubbytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Benjamin Schmidt](https://benschmidt.org/) has developed a number of clever generalizations for Volume reading and *writing*. As a result, you can convert a file structure that is in Pairtree into a Stubbytree file format like below. The temporary directory is just for the example, of course - in your case you want a permanent stubbytree root.\n",
    "\n",
    "```python\n",
    "htid = 'osu.32435018220335'\n",
    "invol = Volume(htid, dir='/data/pairtree_root', id_resolver='pairtree')\n",
    "outvol = Volume(htid, dir='/data/stubby_root', id_resolver='stubbytree', mode='wb')\n",
    "outvol.write(invol) \n",
    "```\n",
    "\n",
    "`invol` can be any Volume that is readable. (By the way, if you're already rewriting files, add `format='parquet'` and write the new ones in the faster alternative Parquet format!)\n",
    "\n",
    "If you're looking for a large batch rewrite of the pairtree files which just moves files around without reading them, the Massive Texts Lab has a scrip to do so: https://github.com/massivetexts/compare-tools/blob/master/scripts/pairtree_to_stubbytree.py. This was written back when Stubbytree was only a lab-specific creation - now that HTRC Extracted Features use Stubbytree, it may just be easier to download your files anew, in the EF2.0 format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volumes can now load files directly\n",
    "\n",
    "As you've seen up to this point, the Feature Reader supports initializing single files through `Volume()` now, while still supporting the approach for feature a larger collection with `FeatureReader()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/loc.ark:/13960/t6737fd9d'>Shakespeare's Merchant of Venice,</a></strong>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f37178bcc18>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Volume('../data/ef2-stubby/loc/a+30679/loc.ark+=13960=t6737fd9d.json.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration through the FeatureReader is still possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['../data/ef2-stubby/pst/0055/pst.000029571581.json.bz2', '../data/ef2-stubby/pst/0053/pst.000068517380.json.bz2']\n",
      "<Volume: A vision : A reissue with the... (1965) by Y>\n",
      "<Volume: Special funds : status of appr... (1990) by P>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "paths = glob.glob('../data/ef2-stubby/pst/**/*bz2', recursive=True)\n",
    "fr = FeatureReader(paths)\n",
    "for vol in fr.volumes():\n",
    "    print(vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volumes hold non-json internal representations\n",
    "\n",
    "The Volume is now made up of four DataFrame: tokencounts, line character counts, section-level features (i.e. the page level features that are provided for header/body/footer), and page-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">body</th>\n",
       "      <th>\"</th>\n",
       "      <th>``</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <th>.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count\n",
       "page section token pos       \n",
       "2    body    \"     ``       1\n",
       "             .     .        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.tokenlist().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th>place</th>\n",
       "      <th>char</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">body</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">begin</th>\n",
       "      <th>F</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count\n",
       "page section place char       \n",
       "2    body    begin F         1\n",
       "                   a         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.line_chars().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>capAlphaSeq</th>\n",
       "      <th>emptyLineCount</th>\n",
       "      <th>lineCount</th>\n",
       "      <th>sentenceCount</th>\n",
       "      <th>tokenCount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>header</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              capAlphaSeq  emptyLineCount  lineCount  sentenceCount  \\\n",
       "page section                                                          \n",
       "1    header             0               0          0              0   \n",
       "     body               0               0          0              0   \n",
       "\n",
       "              tokenCount  \n",
       "page section              \n",
       "1    header            0  \n",
       "     body              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.section_features(section='all').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata is imported from the parser as a Volume property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'mdp.39015028036104',\n",
       " 'schema_version': '1.3',\n",
       " 'date_created': '2016-06-19T18:28:16.1649565Z',\n",
       " 'title': 'Russian short stories, ed. for school use,',\n",
       " 'pub_date': '1919',\n",
       " 'language': 'eng',\n",
       " 'ht_bib_url': 'http://catalog.hathitrust.org/api/volumes/full/htid/mdp.39015028036104.json',\n",
       " 'handle_url': 'http://hdl.handle.net/2027/mdp.39015028036104',\n",
       " 'oclc': ['1456817'],\n",
       " 'imprint': 'Scott, Foresman and company [c1919]',\n",
       " 'names': ['Schweikert, Harry Christian 1877- ed. '],\n",
       " 'classification': {'lcc': ['PZ1.S413 Ru']},\n",
       " 'type_of_resource': 'text',\n",
       " 'issuance': 'monographic',\n",
       " 'genre': ['not fiction'],\n",
       " 'bibliographic_format': 'BK',\n",
       " 'pub_place': 'ilu',\n",
       " 'government_document': False,\n",
       " 'source_institution': 'MIU',\n",
       " 'enumeration_chronology': ' ',\n",
       " 'hathitrust_record_number': '1059466',\n",
       " 'rights_attributes': 'pd',\n",
       " 'access_profile': 'google',\n",
       " 'volume_identifier': 'mdp.39015028036104',\n",
       " 'source_institution_record_number': '001059466',\n",
       " 'isbn': [],\n",
       " 'issn': [],\n",
       " 'lccn': ['19006802'],\n",
       " 'last_update_date': '2007-12-04 09:30:03',\n",
       " 'page_count': 460}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.parser.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, [])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.page_count, vol.issn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative data parsers are supported\n",
    "\n",
    "The bzipped JSON files may not meet all use cases. Developers can now extend basicVolumeParser with their own parsers, which are given to FeatureReader or a Volume with the `parser=...` argument. This will also help scale to future changes in the HTRC's Extracted Features file format.\n",
    "\n",
    "There are two volume parsers included: `jsonVolumeParser` (default), and `parquetVolumeParser`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feature file can hold incomplete data\n",
    "\n",
    "The feature reader is now more robust toward loading data that may be missing parts of speech, or lowercases, or not have the page sections. This can be useful for saving more succinct versions of texts.\n",
    "\n",
    "`Volume.tokenlist()` also now contains a `drop_section` arguments, to drop the 'section' index level. This is a common use case, because most users only keep the 'body' section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for Parquet-based dataset files\n",
    "\n",
    "The current parser enforces a filename convention, and you pass the extensionless file path. Here's what the files look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/parquet/mdp.39015028036104.meta.json',\n",
       " '../data/parquet/mdp.39015028036104.tokens.parquet',\n",
       " '../data/parquet/mdp.39015028036104.section.parquet',\n",
       " '../data/parquet/mdp.39015028036104.chars.parquet']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('../data/parquet/mdp.39015028036104*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need all four - perhaps you just want to load tokencounts and metadata, or even just metadata. The files are lazy-loaded, so if you have all four files but only want to access the metadata, you don't need to hide the other files - just don't call information from them!\n",
    "\n",
    "Loading is done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/mdp.39015028036104'>Russian short stories, ed. for school use,</a></strong> by <em>Schweikert, Harry Christian 1877- ed. </em> (1919, 460 pages) - <code>mdp.39015028036104</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f370c95c550>"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvol = Volume('mdp.39015028036104', format='parquet', dir='../data/parquet')\n",
    "pvol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying format did the trick, though you can also provide a fileHandler directly. Use this for alternative ways of storing/loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href='http://hdl.handle.net/2027/mdp.39015028036104'>Russian short stories, ed. for school use,</a></strong> by <em>Schweikert, Harry Christian 1877- ed. </em> (1919, 460 pages) - <code>mdp.39015028036104</code>"
      ],
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x7f370c95aeb8>"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from htrc_features.parsers import ParquetFileHandler\n",
    "Volume('mdp.39015028036104', dir='../data/parquet', file_handler=ParquetFileHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now a `Volume.save` method for saving to the parquet format (or other formats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mVolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "A wrapper around the 'write' method of all IdResolvers, \n",
       "that allows you to quickly declare a div, a format, and other\n",
       "kwargs.\n",
       "\n",
       "The primary use is for converting the feature files to\n",
       "a more efficient parquet format. By default, only metadata and\n",
       "tokencounts are saved, using the naming convention used by\n",
       "parquetVolumeParser.\n",
       "\n",
       "Saving page features is currently unsupported, as it's an\n",
       "ill-fit for parquet. This is currently just the\n",
       "language-inferences for each page - everything else is in\n",
       "section features (page by body/header/footer).\n",
       "\n",
       "Since Volumes partially support incomplete dataframes, you can\n",
       "pass Volume.tokenlist arguments as a dict with\n",
       "token_kwargs. For example, if you want to save a\n",
       "representation with only body information, drop the 'section'\n",
       "level of the index, and fold part-of-speech counts, you can\n",
       "pass token_kwargs=dict(section='body', drop_section=True,\n",
       "pos=False).\n",
       "\u001b[0;31mFile:\u001b[0m      ~/htrc-feature-reader/htrc_features/feature_reader.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Volume.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, writing to Parquet internally calls `ParquetFileHandler.write`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mParquetFileHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvolume\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindexed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Save the internal representations of feature data to parquet, and the metadata to json,\n",
       "using the naming convention used by ParquetFileHandler.\n",
       "\n",
       "The primary use is for converting the feature files to something more efficient. By default,\n",
       "only metadata and tokencounts are saved.\n",
       "\n",
       "files lists which files you want to get. Default is 'meta', and 'tokens'.\n",
       "Also allowed are 'chars' (character counts) and 'section_features'\n",
       "\n",
       "\n",
       "'volume' is an object of the 'Volume' class which will be used for data. It will\n",
       "almost certainly need to come from a true JSON file.\n",
       "\n",
       "Saving page features is currently unsupported, as it's an ill-fit for parquet. This is currently\n",
       "just the language-inferences for each page - everything else is in section features \n",
       "(page by body/header/footer).\n",
       "\n",
       "Since Volumes partially support incomplete dataframes, you can pass Volume.tokenlist arguments\n",
       "as a dict with token_kwargs. For example, if you want to save a representation with only body\n",
       "information, drop the 'section' level of the index, and fold part-of-speech counts, you can pass\n",
       "token_kwargs=dict(section='body', drop_section=True, pos=False).\n",
       "\u001b[0;31mFile:\u001b[0m      ~/htrc-feature-reader/htrc_features/parsers.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ParquetFileHandler.write?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, only the tokens and metadata are saved. You can also save a partial tokenlist if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Page was stupefied\n",
    "\n",
    "The Page object was stupefied - it reaches up to the associated Volume for all of it's functionality now, and all the page-level Volume methods have a page_select argument for selecting only a single page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
