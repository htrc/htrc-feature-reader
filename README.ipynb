{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTRC-Features\n",
    "=============\n",
    "\n",
    "Tools for working with the [HTRC Extracted Features dataset](https://sharc.hathitrust.org/features), a dataset of page-level text analysis features extracted from from 4.8 million public domain volumes.\n",
    "\n",
    "This library provides a `FeatureReader` for parsing files, which are handled as `Volume` objects with collections of `Page` objects. Volumes provide access to metadata (e.g. language), volume-wide feature information (e.g. token counts), and access to Pages. Pages allow you to easily parse page-level features, particularly token lists.\n",
    "\n",
    "This library makes heavy use of [Pandas](pandas.pydata.org), returning many data representations as DataFrames. This is the leading way of dealing with structured data in Python, so this library doesn't try to reinvent the wheel. Since refactoring around Pandas, the primary benefit of using the HTRC Feature Reader is performance: reading the json structures and parsing them is generally faster than custom code. You also get convenient access to common information, such as case-folded token counts or part-of-page specific character counts. Details of the public methods provided by this library can be found in the [HTRC Feature Reader docs](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html).\n",
    "\n",
    "**Table of Contents**: [Installation](#Installation) | [Usage](#Usage) | \n",
    "[Additional Notes](#Additional-Notes)\n",
    "\n",
    "**Links**: \n",
    "[HTRC Feature Reader Documentation](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html) | [HTRC Extracted Features Dataset](https://sharc.hathitrust.org/features)\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install,\n",
    "\n",
    "    pip install htrc-feature-reader\n",
    "\n",
    "That's it! This library is written for Python 2.7 and 3.0+. Optional: [installing the development version](#Installing-the-development-version). For Python beginners, you'll need [pip](https://pip.pypa.io/en/stable/installing/).\n",
    "\n",
    "Given the nature of data analysis, using iPython with Jupyter notebooks for preparing your scripts interactively is a recommended convenience. Most basically, it can be installed with `pip install ipython[notebook]` and run with `ipython notebook` from the command line, which starts a session that you can access through your browser. If this doesn't work, consult the [iPython documentation](http://ipython.readthedocs.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Reading feature files\n",
    "\n",
    "The easiest way to start using this library is to use the [FeatureReader](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.FeatureReader) interface, which takes a list of paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvd.32044010273894 - The ballet dancer, and On guard,\n",
      "njp.32101068970662 - Seven years, and other tales / by Julia Kavanagh.\n",
      "nyp.33433074811310 - June / by Edith Barnard Delano ; with illustrations.\n",
      "nyp.33433075749246 - You never know your luck; being the story of a matrimonial deserter, by Gilbert Parker ... illustrated by W.L. Jacobs.\n",
      "mdp.39015028036104 - Russian short stories, ed. for school use,\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from htrc_features import FeatureReader\n",
    "paths = glob.glob('data/PZ-volumes/*.json.bz2')\n",
    "# Here we're loading five paths, for brevity\n",
    "fr = FeatureReader(paths[:5])\n",
    "for vol in fr.volumes():\n",
    "    print(\"%s - %s\" % (vol.id, vol.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating on `FeatureReader` returns `Volume` objects. This is simply an easy way to access `feature_reader.volumes()`.\n",
    "Wherever possible, this library tries not to hold things in memory, so most of the time you want to iterate rather than casting to a list.\n",
    "In addition to memory issues, since each volume needs to be read from a file and initialized, it will be slow. \n",
    "_Woe to whomever tries `list(FeatureReader.volumes())`_.\n",
    "\n",
    "The method for creating a path list with 'glob' is just one way to do so.\n",
    "For large sets, it's better to just have a text file of your paths, and read it line by line.\n",
    "\n",
    "The feature reader also has a useful method, `multiprocessing(map_func)`, for chunking a running functions across multiple processes.\n",
    "This is an advanced feature, but extremely helpful for any large-scale processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to iterating on `feature_reader.volumes()`, there is a convenient function to grab the first volume in a feature reader. This helps in testing code, and is what we'll do to continue this introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<htrc_features.feature_reader.Volume at 0x17f083be828>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = fr.first()\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "\n",
    "A [Volume](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume) contains information about the current work and access to the pages of the work. All the metadata fields from the HTRC JSON file are accessible as properties of the volume object, including _title_, _language_, _imprint_, _oclc_, _pubDate_, and _genre_. The main identifier _id_ and _pageCount_ are also accessible, and you can find the URL for the Full View of the text in the HathiTrust Digital Library - if it exists - with `vol.handle_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volume hvd.32044010273894 is a 284 page text written in eng. You can doublecheck at http://hdl.handle.net/2027/hvd.32044010273894'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Volume %s is a %s page text written in %s. You can doublecheck at %s\" % (vol.id, vol.page_count,\n",
    "                                                                          vol.language, vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a convenience, `Volume.year` returns `Volume.pub_date`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1901 == 1901'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%s == %s\" % (vol.pub_date, vol.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Volume` objects have an page genrator method for pages, through `Volume.pages()`. Iterating through pages using this generator only keeps one page at a time in memory, and again it is preferable to reading all the pages into the list at once. Unlike volumes, your computer can probably hold all the pages of a single volume in memory, so it is not dire if you try to read them into a list.\n",
    "\n",
    "Like with the `FeatureReader`, you can also access the page generator by iterating directly on the object (i.e. `for page in vol`). Python beginners may find that using `vol.pages()` is more clear as to what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<page 00000016 of volume hvd.32044010273894>\n"
     ]
    }
   ],
   "source": [
    "# Let's skip ahead some pages\n",
    "i = 0\n",
    "for page in vol:\n",
    "    # Same as `for page in vol.pages()`\n",
    "    i += 1\n",
    "    if i >= 16:\n",
    "        break\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to pass arguments to page initialization, such as changing the page's default section from 'body' to 'group' (which returns header+footer+body), it can be done with `for page in vol.pages(default_section='group')`.\n",
    "     \n",
    "Finally, if the minimal metadata included with the extracted feature files is insufficient, you can fetch the HTRC's metadata record from the Solr Proxy with `vol.metadata`.\n",
    "Remember that this calls the HTRC servers for each volume, so can add considerable overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York, and London, Harper & brothers, 1901\n",
      "London : Hurst and Blackett, 1860\n",
      "Boston ; New York : Houghton Mifflin Company, 1916 (Cambridge : The Riverside Press)\n",
      "New York, George H. Doran Company [1914]\n",
      "Chicago, New York, Scott, Foresman and company [c1919]\n"
     ]
    }
   ],
   "source": [
    "for vol in fr.volumes():\n",
    "    print(vol.metadata['published'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METADATA FIELDS: language, publication_place, title_a, ht_id, id, sdrnum, topic_subject, htrc_gender, topic, fullrecord, author_only, htrc_volumePageCountBin, title_top, format, htrc_volumeWordCountBin, geographic, publisher, mainauthor, callnumber, genre, topicStr, htrc_charCount, country_of_pub, htrc_wordCount, htrc_genderMale, publishDate, authorSort, oclc, publishDateRange, htsource, title_ab, published, author_top, htrc_pageCount, lccn, title, _version_, callnosort, author\n"
     ]
    }
   ],
   "source": [
    "print(\"METADATA FIELDS: \" + \", \".join(vol.metadata.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_At large-scales, using `vol.metadata` is an impolite and inefficient amount of server pinging; there are better ways to query the API than one volume at a time. Read about the [HTRC Solr Proxy](https://wiki.htrc.illinois.edu/display/COM/Solr+Proxy+API+User+Guide)._\n",
    "\n",
    "Another source of bibliographic metadata is the HathiTrust Bib API. You can access this information through the URL returned with `vol.ht_bib_url`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://catalog.hathitrust.org/api/volumes/full/htid/mdp.39015028036104.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.ht_bib_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volumes also have direct access to volume-wide info of features stored in pages. For example, you can get a list of words per page through [Volume.tokens_per_page()](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume.tokens_per_page). We'll discuss these features [below](#Volume-stats-collecting), after looking first at Pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pages\n",
    "\n",
    "A page contains the meat of the HTRC's extracted features, including information for:\n",
    "\n",
    "- Part of speech tagged token counts, through `Page.tokenlist()`\n",
    "- Counts of the characters occurred at the start and end of physical lines, though `Page.lineCounts()`\n",
    "- Sentence counts, line counts (referring to the physical line on the page)\n",
    "- And more, seen in the docs for [Page](http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The body has 30 lines, 0 empty lines, and 9 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"The body has %s lines, %s empty lines, and %s sentences\" % (page.line_count(),\n",
    "                                                                   page.empty_line_count(),\n",
    "                                                                   page.sentence_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the HTRC provides information by header/body/footer, most methods take a `section=` argument. If not specified, this defaults to `\"body\"`, or whatever argument is supplied to `Page.default_section`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 tokens in the default section, body\n",
      "3 tokens in the header\n",
      "0 tokens in the footer\n"
     ]
    }
   ],
   "source": [
    "print(\"%s tokens in the default section, %s\" % (page.token_count(), page.default_section))\n",
    "print(\"%s tokens in the header\" % (page.token_count(section='header')))\n",
    "print(\"%s tokens in the footer\" % (page.token_count(section='footer')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two special arguments that can be given to `section`: `\"all\"` and \"`group`\". 'all' returns information for each section separately, when appropriate, while 'group' returns information for all header, body, and footer combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 tokens on the full page\n"
     ]
    }
   ],
   "source": [
    "print(\"%s tokens on the full page\" % (page.token_count(section='group')))\n",
    "assert(page.token_count(section='group') == (page.token_count(section='header') +\n",
    "                                             page.token_count(section='body') + \n",
    "                                             page.token_count(section='footer')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the most part, the properties of the `Page` and `Volume` objects aligns with the names in the HTRC Extracted Features schema, except they are converted to follow [Python naming conventions](https://google.github.io/styleguide/pyguide.html?showone=Naming#Naming): converting the `CamelCase` of the schema to `lowercase_with_underscores`. E.g. `beginLineChars` from the HTRC data is accessible as `Page.begin_line_chars`.\n",
    "\n",
    "## The fun stuff: playing with token counts and character counts\n",
    "\n",
    "Token counts are returned by `Page.tokenlist()`. By default, part-of-speech tagged, case-sensitive counts are returned for the body.\n",
    "\n",
    "The token count information is returned as a DataFrame with a MultiIndex (page, section, token, and part of speech) and one column (count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           count\n",
      "page section token    pos       \n",
      "16   body    !        .        1\n",
      "             '        ''       1\n",
      "             'Flowers NNS      1\n"
     ]
    }
   ],
   "source": [
    "print(page.tokenlist()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Page.tokenlist()` can be manipulated in various ways. You can case-fold, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            count\n",
      "page section lowercase pos       \n",
      "16   body    ancient   JJ       1\n",
      "             and       CC      12\n",
      "             any       DT       1\n"
     ]
    }
   ],
   "source": [
    "df = page.tokenlist(case=False)\n",
    "print(df[15:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can combine part of speech counts into a single integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       count\n",
      "page section token          \n",
      "16   body    Naples        1\n",
      "             November      1\n",
      "             October       1\n"
     ]
    }
   ],
   "source": [
    "df = page.tokenlist(pos=False)\n",
    "print(df[15:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section arguments are valid here: 'header', 'body', 'footer', 'all', and 'group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        count\n",
      "page section lowercase       \n",
      "16   header  ballet         1\n",
      "             dancer         1\n",
      "             the            1\n"
     ]
    }
   ],
   "source": [
    "df = page.tokenlist(section=\"header\", case=False, pos=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultiIndex makes it easy to slice the results, and it is althogether more memory-efficient. If you are new to Pandas DataFrames, you might find it easier to learn by converting the index to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               count\n",
      "page section token        pos       \n",
      "16   body    benefactress NN       1\n",
      "             bitterness   NN       1\n",
      "             case         NN       1\n",
      "With index reset: \n",
      "   page section         token pos  count\n",
      "0    16    body  benefactress  NN      1\n",
      "1    16    body    bitterness  NN      1\n"
     ]
    }
   ],
   "source": [
    "df = page.tokenlist()\n",
    "# Slicing on Multiindex: get all Signular or Mass Nouns (NN)\n",
    "idx = pd.IndexSlice\n",
    "nouns = df.loc[idx[:,:,:,'NN'],]\n",
    "print(nouns[:3])\n",
    "print(\"With index reset: \")\n",
    "print(nouns.reset_index()[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer not to use Pandas, you can always convert the object, with methods like `to_dict` and `to_csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': {(16, 'body', '!', '.'): 1,\n",
       "  (16, 'body', \"'\", \"''\"): 1,\n",
       "  (16, 'body', \"'Flowers\", 'NNS'): 1}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get just the unique tokens, `Page.tokens` provides them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', \"'\", \"'Flowers\", \"'s\", ',', '.', '6']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.tokens()[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to token lists, you can also access `Page.begin_line_chars` and `Section.end_line_chars`, which are DataFrames of character counts that occur at the start or end of a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume stats collecting\n",
    "\n",
    "The Volume object has a number of methods for collecting information from all its pages.\n",
    "\n",
    "`Volume.tokenlist()` works identically the page tokenlist method, except it returns information for the full volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        count\n",
      "page section lowercase       \n",
      "38   body    she            1\n",
      "39   body    she            1\n",
      "42   body    she            1\n"
     ]
    }
   ],
   "source": [
    "# Print case-insensitive occurrances of the word `she`\n",
    "all_vol_token_counts = vol.tokenlist(pos=False, case=False)\n",
    "print(all_vol_token_counts.loc[idx[:,'body', 'she'],][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a Volume-wide tokenlist is not crunched until you need it, then it will stay cached in case you need it. If you try to access `Page.tokenlist()` _after_ accessing `Volume.tokenlist()`, the Page object will return that page from the Volume's cached representation, rather than preparing it itself.\n",
    "\n",
    "`Volume.tokens()`, and `Volume.tokens_per_page()` give easy access to the full vocabulary of the volume, and the token counts per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', '.', ':', 'Fred', 'Newton', 'Scott', 'gift', 'i', 'ii', 'iiiiISI']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol.tokens()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer a DataFrame structured like a term-document matrix (where pages are the 'documents'), `vol.term_page_freqs()` will return it.\n",
    "\n",
    "By default, this returns a page-frequency rather than term-frequency, which is to say it counts `1` when a term occurs on a page, regardless of how much it occurs on that page. For a term frequency, pass `page_freq=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token  the  and   is   he  she\n",
      "page                          \n",
      "10     0.0  1.0  0.0  0.0  0.0\n",
      "11     1.0  1.0  1.0  0.0  0.0\n",
      "token   the  and   is   he  she\n",
      "page                           \n",
      "10      0.0  1.0  0.0  0.0  0.0\n",
      "11     22.0  7.0  4.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "a = vol.term_page_freqs()\n",
    "print(a.loc[10:11,['the','and','is','he', 'she']])\n",
    "a = vol.term_page_freqs(page_freq=False)\n",
    "print(a.loc[10:11,['the','and','is', 'he', 'she']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume.term_page_freqs provides a wide DataFrame resembling a matrix, where terms are listed as columns, pages are listed as rows, and the values correspond to the term frequency (or page page frequency with `page_freq=true`).\n",
    "Volume.term_volume_freqs() simply sums these.\n",
    " \n",
    "### Multiprocessing\n",
    "\n",
    "For faster processing, you can write a mapping function for acting on volumes, then pass it to `FeatureReader.multiprocessing`.\n",
    "This sends out the function to a different process per volume, spawning (CPU_CORES-1) processes at a time.\n",
    "The map function receives the feature_reader and a volume path as a tuple, and needs to initialize the volume.\n",
    "\n",
    "Here's a simple example that returns the term counts for each volume (take note of the first two lines of the function):\n",
    "\n",
    "```python\n",
    "def printTokenList(args):\n",
    "    fr, path = args\n",
    "    vol = fr.create_volume(path)\n",
    "    return ('tokens', vol.tokens)\n",
    "\n",
    "fr  = FeatureReader(paths)\n",
    "all_tokens = []\n",
    "mapper = fr.multiprocessing(printTokenList)\n",
    "for key, result in mapper:\n",
    "    all_tokens = all_tokens + result\n",
    "set(all_tokens)\n",
    "```\n",
    "\n",
    "Some rules: results must be serializeable, and the map_func must be accessible from __main__ (basically: no dynamic functions: they should be written plainly in your script).\n",
    "\n",
    "The results are collected and returned together, so you don't want a feature reader with all 4.8 million files, because the results will be too much memory (depending on how big your result is).\n",
    "Instead, it easier to initialize feature readers for smaller batches.\n",
    "\n",
    "#### GNU Parallel\n",
    "As an alternative to multiprocessing in Python, my preference is to have simpler Python scripts and to use GNU Parallel on the command line. To do this, you can set up your Python script to take variable length arguments of feature file paths, and to print to stdout.\n",
    "\n",
    "This psuedo-code shows how that you'd use parallel, where the number of parallel processes is 90% the number of cores, and 50 paths are sent to the script at a time (if you send too little at a time, the initialization time of the script can add up).\n",
    "\n",
    "```bash\n",
    "find feature-files/ -name '*json.bz2' | parallel --eta --jobs 90% -n 50 python your_script.py >output.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "### Installing the development version\n",
    "\n",
    "    git clone https://github.com/htrc/htrc-feature-reader.git\n",
    "    cd htrc-feature-reader\n",
    "    python setup.py install\n",
    "\n",
    "### Iterating through the JSON files\n",
    "\n",
    "If you need to do fast, highly customized processing without instantiating Volumes, FeatureReader has a convenient generator for getting the raw JSON as a Python dict: `fr.jsons()`. This simply does the file reading, optional decompression, and JSON parsing.\n",
    "\n",
    "### Getting the Rsync URL\n",
    "\n",
    "If you have a HathiTrust Volume ID and want to be able to download the features for a specific book, `hrtc_features.utils` contains an [id_to_rsync](http://htrc.github.io/htrc-feature-reader/htrc_features/utils.m.html#htrc_features.utils.id_to_rsync) function. This uses the [pairtree](http://pythonhosted.org/Pairtree/) library but has a fallback written with that library is not installed, since it isn't compatible with Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miun/pairtree_root/ad/x6/30/0,/00/01/,0/01/adx6300,0001,001/adx6300,0001,001.json.bz2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from htrc_features import utils\n",
    "utils.id_to_rsync('miun.adx6300.0001.001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [ID to Rsync notebook](examples/ID_to_Rsync_Link.ipynb) for more information on this format and on Rsyncing lists of urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Features\n",
    "\n",
    "In the beta Extracted Features release, schema 2.0, a few features were separated out to an advanced files. However, *this designation is no longer present starting with schema 3.0*, meaning information like `beginLineChars`, `endLineChars`, and `capAlphaSeq` are always available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 0, 0, 0, 4, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the longest sequence of capital letter on each page?\n",
    "vol.cap_alpha_seqs()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         count\n",
      "page section place char       \n",
      "2    body    end   -         1\n",
      "                   :         1\n",
      "                   I         1\n",
      "                   f         1\n",
      "                   t         1\n"
     ]
    }
   ],
   "source": [
    "end_line_chars = vol.end_line_chars()\n",
    "print(end_line_chars.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         count\n",
      "page section place char       \n",
      "45   body    end   !         1\n",
      "75   body    end   !         1\n",
      "77   body    end   !         1\n",
      "91   body    end   !         1\n",
      "92   body    end   !         1\n"
     ]
    }
   ],
   "source": [
    "# Find pages that have lines ending with \"!\"\n",
    "idx = pd.IndexSlice\n",
    "print(end_line_chars.loc[idx[:,:,:,'!'],].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "This library is meant to be compatible with Python 3.2+ and Python 2.7+. Tests are written for py.test and can be run with `setup.py test`, or directly with `python -m py.test -v`.\n",
    "\n",
    "If you find a bug, leave an issue on the issue tracker, or contact Peter Organisciak at `organisciak+htrc@gmail.com`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
